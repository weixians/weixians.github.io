<section id="nice" data-tool="mdnice编辑器" data-website="https://www.mdnice.com" style="font-size: 16px; color: black; padding: 0 10px; line-height: 1.6; word-spacing: 0px; letter-spacing: 0px; word-break: break-word; word-wrap: break-word; text-align: left; font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, 'PingFang SC', Cambria, Cochin, Georgia, Times, 'Times New Roman', serif;"><h1 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 24px;"><span class="prefix" style="display: none;"></span><span class="content">强化学习 (Reinforcement Learning)</span><span class="suffix"></span></h1>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">强推：<a href="https://www.cnblogs.com/pinard/category/1254674.html" style="text-decoration: none; color: #1e6bb8; word-wrap: break-word; font-weight: bold; border-bottom: 1px solid #1e6bb8;">刘建平的强化学习教程</a>，本文内容亦大部分整理自此。本文更多的是当做工具书来快速查找自己想要的内容，而非对所有内容详细介绍。</p>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">下一篇：<a href="https://zhuanlan.zhihu.com/p/382224733" style="text-decoration: none; color: #1e6bb8; word-wrap: break-word; font-weight: bold; border-bottom: 1px solid #1e6bb8;">《强化学习基础》- 时序差分(TD)、SARSA、Q-learning</a></p>
<h3 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 20px;"><span class="prefix" style="display: none;"></span><span class="content"><span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\varepsilon" style="display: block; margin: 0 auto; max-width: 100%;"></span>-greedy</span><span class="suffix" style="display: none;"></span></h3>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="a_{t}=\left\{\begin{array}{l}a_{t}^{*} \text { with probability } 1-\varepsilon \\ \text { random action with probability } \varepsilon\end{array}\right.
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">另一种表示法，二者意思一样：</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\pi(a \mid s)=\left\{\begin{array}{ll}
\varepsilon / m+1-\varepsilon &amp; \text { if } a^{*}=\arg \max _{a \in A} Q(s, a) \\
\varepsilon / m &amp; \text { else }
\end{array}\right.
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<h2 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 22px;"><span class="prefix" style="display: none;"></span><span class="content">强化学习8大要素</span><span class="suffix"></span></h2>
<ol data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">环境的状态<strong style="font-weight: bold; color: black;">S</strong>， t时刻环境的状态<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="S_t" style="display: block; margin: 0 auto; max-width: 100%;"></span>是它的环境状态集中某一个状态。</p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">个体的动作<strong style="font-weight: bold; color: black;">A</strong>， t时刻个体采取的动作<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="A_t" style="display: block; margin: 0 auto; max-width: 100%;"></span>是它的动作集中某一个动作。</p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">环境的奖励<strong style="font-weight: bold; color: black;">R</strong>， t时刻个体在状态St采取的动作<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="A_t" style="display: block; margin: 0 auto; max-width: 100%;"></span>对应的奖励<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="R_{t+1}" style="display: block; margin: 0 auto; max-width: 100%;"></span>会在t+1时刻得到。</p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">个体的策略(policy) <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi" style="display: block; margin: 0 auto; max-width: 100%;"></span>， 它代表个体采取动作的依据，即个体会依据策略π来选择动作。最常见的策略表达方式是一个条件概率分布<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi(a|s)" style="display: block; margin: 0 auto; max-width: 100%;"></span>, 即在状态s时采取动作a的概率。即<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi(a|s)=P(A_t=a|S_t=s)" style="display: block; margin: 0 auto; max-width: 100%;"></span>。概率大的动作被个体选择的概率较高。</p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">个体在策略π和状态s时，采取行动后的价值（value），一般用<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_\pi(s)" style="display: block; margin: 0 auto; max-width: 100%;"></span>表示。这个价值一般是一个期望函数。虽然当前动作会给一个延时奖励<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="R_{t+1}" style="display: block; margin: 0 auto; max-width: 100%;"></span>,但是光看这个延时奖励是不行的，因为当前的延时奖励高，不代表到了t+1,t+2,...时刻的后续奖励也高。比如下象棋，我们可以某个动作可以吃掉对方的车，这个延时奖励是很高，但是接着后面我们输棋了。此时吃车的动作奖励值高但是价值并不高。因此我们的价值要综合考虑当前的延时奖励和后续的延时奖励。价值函数<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_\pi(s)" style="display: block; margin: 0 auto; max-width: 100%;"></span>一般可以表示为下式，不同的算法会有对应的一些价值函数变种，但思路相同。：</p>
</section></li></ol>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="v_{\pi}(s)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right)
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<ol start="6" data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">奖励衰减因子 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\gamma" style="display: block; margin: 0 auto; max-width: 100%;"></span>，在[0，1]之间。如果为0，则是贪婪法，即价值只由当前延时奖励决定，如果是1，则所有的后续状态奖励和当前奖励一视同仁。大多数时候，我们会取一个0到1之间的数字，即当前延时奖励的权重比后续奖励的权重大。</p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">环境的状态转化模型 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="P^a_{ss′}" style="display: block; margin: 0 auto; max-width: 100%;"></span>，可以理解为一个概率状态机，它可以表示为一个概率模型，即在状态s下采取动作a,转到下一个状态s′的概率。</p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">探索率 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\varepsilon" style="display: block; margin: 0 auto; max-width: 100%;"></span>，这个比率主要用在强化学习训练迭代过程中，由于我们一般会选择使当前轮迭代价值最大的动作，但是这会导致一些较好的但我们没有执行过的动作被错过。因此我们在训练选择最优动作时，会有一定的概率 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\varepsilon" style="display: block; margin: 0 auto; max-width: 100%;"></span> 不选择使当前轮迭代价值最大的动作，而选择其他的动作。</p>
</section></li></ol>
<h2 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 22px;"><span class="prefix" style="display: none;"></span><span class="content">贝尔曼方程 (Bellman equation)</span><span class="suffix"></span></h2>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\begin{aligned}
v_{\pi}(s) &amp;=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right) \\
&amp;=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) \mid S_{t}=s\right) \\
&amp;=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right) \\
&amp;=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right)
\end{aligned}
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">从上面方程可以看出，在 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="t" style="display: block; margin: 0 auto; max-width: 100%;"></span> 时刻的状念 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="S_{t}" style="display: block; margin: 0 auto; max-width: 100%;"></span> 和 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="t+1" style="display: block; margin: 0 auto; max-width: 100%;"></span> 时亥的状交 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="S_{t+1}" style="display: block; margin: 0 auto; max-width: 100%;"></span> 是满足递推关桃的，因此可以得出，</p>
<ol data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">价值函数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_{\pi}(s)" style="display: block; margin: 0 auto; max-width: 100%;"></span> 的贝尔曼方程：</section></li></ol>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="v_{\pi}(s)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right)
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<ol start="2" data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">动作价值函数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="q_{\pi}(s, a)" style="display: block; margin: 0 auto; max-width: 100%;"></span> 的贝尔曼方程：</section></li></ol>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="q_{\pi}(s, a)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) \mid S_{t}=s, A_{t}=a\right)
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<h2 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 22px;"><span class="prefix" style="display: none;"></span><span class="content">价值函数(state value fucntion) 与 动作价值函数(action-state value function)</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">从贝尔曼公式得到二者的计算公式：</p>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">价值函数：</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\begin{aligned}
v_{\pi}(s)&amp;=\sum_{a \in A} \pi(a \mid s) q_{\pi}(s, a) \\
&amp;=\sum_{a \in A} \pi(a \mid s)\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right) 
\end{aligned}
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">动作价值函数：</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\begin{aligned}
q_{\pi}(s, a)&amp;=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\\
&amp;=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)
\end{aligned}
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">最优价值函数：</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\begin{aligned}
v_{*}(s)
&amp;=\max _{a} q_{*}(s, a)
\\
&amp;=\max _{a}\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)\right) \\
\end{aligned}
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">最优动作价值函数：</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\begin{aligned}
q_{*}(s, a)
&amp;=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)\\
&amp;=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)
\end{aligned}
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<h2 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 22px;"><span class="prefix" style="display: none;"></span><span class="content">动态规划 (Dynamic programming)</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">强化学习的两个基本问题：</p>
<ol data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><strong style="font-weight: bold; color: black;">预测</strong>，即给定强化学习的<strong style="font-weight: bold; color: black;">6</strong>个要素，求解该策略的状态价值函数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v(\pi)" style="display: block; margin: 0 auto; max-width: 100%;"></span>:
<ul style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: disc;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">状态集 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="S" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">动作集 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="A" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">模型状态转化概率矩阵 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="P" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">即时奖励 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="R" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">衰减因子 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\gamma" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">给定策略 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li></ul>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><strong style="font-weight: bold; color: black;">控制</strong>, 即给定强化学习的<strong style="font-weight: bold; color: black;">5</strong>个要素，求解最优的状态价值函数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_{*}" style="display: block; margin: 0 auto; max-width: 100%;"></span> 和最优策略 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi_{*}" style="display: block; margin: 0 auto; max-width: 100%;"></span>：
<ul style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: disc;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">状态集 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="S" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">动作集 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="A" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">模型状态转化概率矩阵 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="P" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">即时奖励 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="R" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">衰减因子 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\gamma" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li></ul>
</section></li></ol>
<h4 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 18px;"><span class="prefix" style="display: none;"></span><span class="content">求解预测问题</span><span class="suffix" style="display: none;"></span></h4>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">即**策略评估 (Policy Evaluation)**，使用动态规划来求解强化学习的预测问题。</p>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">基本思路：从任意一个状态价值函数开始，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。</p>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">假设我们在第 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="k" style="display: block; margin: 0 auto; max-width: 100%;"></span> 轮迭代已经计算出了所有的状态的状态价值, 那么在第 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="k+1" style="display: block; margin: 0 auto; max-width: 100%;"></span> 轮我们可以利用第 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="k" style="display: block; margin: 0 auto; max-width: 100%;"></span> 轮计算出的状态价值计算出第 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="k+1" style="display: block; margin: 0 auto; max-width: 100%;"></span> 轮的状态价值。这是通过贝尔曼方程来完成的，即：</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="v_{k+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right)
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<h4 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 18px;"><span class="prefix" style="display: none;"></span><span class="content">求解控制问题</span><span class="suffix" style="display: none;"></span></h4>
<ol data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;"><strong style="font-weight: bold; color: black;">策略迭代(Policy Iteration)</strong>
<img src="https://raw.githubusercontent.com/weixians/image_bed/master/20210618/policy_iteration.56c9q3be0o00.png" alt="policy_iteration" style="display: block; margin: 0 auto; max-width: 100%;"></p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;"><strong style="font-weight: bold; color: black;">价值迭代(Value Iteration)</strong></p>
<ul style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: disc;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">和策略迭代相比，我们没有等到状态价值收敛才调整策略，而是随着状态价值的迭代及时调整策略, 这样可以大大减少迭代次数。此时我们的状态价值的更新方法也和策略迭代不同。现在的贝尔曼方程迭代式子如下：</section></li></ul>
<span class="span-block-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_{k+1}(s)=\max _{a \in A}\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right)
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<ul style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: disc;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">算法伪代码：
<img src="https://raw.githubusercontent.com/weixians/image_bed/master/20210618/value_iteration~2.55difzpc4ow0.png" alt="value_iteration~2" style="display: block; margin: 0 auto; max-width: 100%;"></section></li></ul>
</section></li></ol>
<h2 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 22px;"><span class="prefix" style="display: none;"></span><span class="content">蒙特卡罗法 (Monte Carlo)</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型P都无法知道，这时动态规划法根本没法使用。</p>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">蒙特卡罗法是一种不基于模型(即概率转换模型 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="P" style="display: block; margin: 0 auto; max-width: 100%;"></span>)的强化学习方法。</p>
<h4 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 18px;"><span class="prefix" style="display: none;"></span><span class="content">问题定义 (少了 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="P" style="display: block; margin: 0 auto; max-width: 100%;"></span> )</span><span class="suffix" style="display: none;"></span></h4>
<ol data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><strong style="font-weight: bold; color: black;">预测</strong>，即给定强化学习的<strong style="font-weight: bold; color: black;">5</strong>个要素，求解该策略的状态价值函数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v(\pi)" style="display: block; margin: 0 auto; max-width: 100%;"></span>:
<ul style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: disc;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">状态集 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="S" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">动作集 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="A" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">即时奖励 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="R" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">衰减因子 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\gamma" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">给定策略 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li></ul>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><strong style="font-weight: bold; color: black;">控制</strong>, 即给定强化学习的<strong style="font-weight: bold; color: black;">5</strong>个要素，求解最优的状态价值函数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_{*}" style="display: block; margin: 0 auto; max-width: 100%;"></span> 和最优策略 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi_{*}" style="display: block; margin: 0 auto; max-width: 100%;"></span>：
<ul style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: disc;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">状态集 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="S" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">动作集 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="A" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">即时奖励 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="R" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">衰减因子 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\gamma" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">探索率 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\varepsilon" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li></ul>
</section></li></ol>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">蒙特卡罗法通过采样若干<strong style="font-weight: bold; color: black;">经历完整</strong>的状态序列(episode)来估计状态的真实价值。<u>所谓的经历完整，就是这个序列必须是达到终点的</u>。 比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。</p>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">从特卡罗法法的特点来说，一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。</p>
<h4 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 18px;"><span class="prefix" style="display: none;"></span><span class="content">求解预测问题/策略评估</span><span class="suffix" style="display: none;"></span></h4>
<ol data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">给定策略 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi" style="display: block; margin: 0 auto; max-width: 100%;"></span> 的完整有T个状态的状态序列如下: <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \ldots S_{t}, A_{t}, R_{t+1}, \ldots R_{T}, S_{T}" style="display: block; margin: 0 auto; max-width: 100%;"></span></p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">再写一遍上面提到的状态价值函数：<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_{\pi}(s)=\mathbb{E}_{\pi}\left(G_{t} \mid S_{t}=s\right)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right)" style="display: block; margin: 0 auto; max-width: 100%;"></span></p>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;"><p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解，也就是：</p>
</section></li></ol>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\begin{gathered}
G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \gamma^{T-t-1} R_{T} \\
v_{\pi}(s) \approx \text { average }\left(G_{t}\right), s . t . S_{t}=s
\end{gathered}
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<blockquote class="multiquote-1" data-tool="mdnice编辑器" style="border: none; display: block; font-size: 0.9em; overflow: auto; overflow-scrolling: touch; border-left: 3px solid rgba(0, 0, 0, 0.4); background: rgba(0, 0, 0, 0.05); color: #6a737d; padding-top: 10px; padding-bottom: 10px; padding-left: 20px; padding-right: 10px; margin-bottom: 20px; margin-top: 20px;">
<p style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0px; color: black; line-height: 26px;">注意：这里的<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="G_t" style="display: block; margin: 0 auto; max-width: 100%;"></span>是长期回报，称为<strong style="font-weight: bold; color: black;">收获(Return)<strong style="font-weight: bold; color: black;">，而<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="R_t" style="display: block; margin: 0 auto; max-width: 100%;"></span>指的是在某个状态下采取动作得到的奖励，称为</strong>奖励(Reward)</strong></p>
</blockquote>
<ol start="4" data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">优化：
<ol style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">同样一个状态可能在一个完整的状态序列中重复出现，此时计算return有两种方式：
<ol style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">首次访问(first visit)蒙特卡罗法。仅把状态序列中第一次出现该状态时的收获值纳入到收获平均值的计算中；</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">每次访问(every visit)蒙特卡罗法。针对一个状态序列中每次出现的该状态，都计算对应的收获值并纳入到收获平均值的计算中</section></li></ol>
</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">累进更新平均值（incremental mean)。上面的average的公式意味着要保存所有该状态的收获值之和最后取平均。这样浪费了太多的存储空间。一个较好的方法是在迭代计算收获均值，即每次保存上一轮迭代得到的收获均值与次数，当计算得到当前轮的收获时，即可计算当前轮收获均值和次数。通过下面的公式就很容易理解这个过程：</section></li></ol>
</section></li></ol>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\mu_{k}=\frac{1}{k} \sum_{j=1}^{k} x_{j}=\frac{1}{k}\left(x_{k}+\sum_{j=1}^{k-1} x_{j}\right)=\frac{1}{k}\left(x_{k}+(k-1) \mu_{k-1}\right)=\mu_{k-1}+\frac{1}{k}\left(x_{k}-\mu_{k-1}\right)
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">这样上面的状态价值公式就可以改写成:</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\begin{gathered}
N\left(S_{t}\right)=N\left(S_{t}\right)+1 \\
V\left(S_{t}\right)=V\left(S_{t}\right)+\frac{1}{N\left(S_{t}\right)}\left(G_{t}-V\left(S_{t}\right)\right)
\end{gathered}
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">这样我们无论数据量是多还是少, 算法需要的内存基本是固定的。有时候, 尤其是海量数据做分布式迭代的时候, 我们可能无法准确计算当前的次数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="N\left(S_{t}\right)" style="display: block; margin: 0 auto; max-width: 100%;"></span>, 这时我们可以用一个系数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\alpha" style="display: block; margin: 0 auto; max-width: 100%;"></span> 来代替, 即:</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="V\left(S_{t}\right)=V\left(S_{t}\right)+\alpha\left(G_{t}-V\left(S_{t}\right)\right)
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">对于动作价值函数 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="Q\left(S_{t}, A_{t}\right)" style="display: block; margin: 0 auto; max-width: 100%;"></span>, 也是类似的, 比如对上面最后一个式子，动作价值函数版本为:</p>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="Q\left(S_{t}, A_{t}\right)=Q\left(S_{t}, A_{t}\right)+\alpha\left(G_{t}-Q\left(S_{t}, A_{t}\right)\right)
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<h4 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 18px;"><span class="prefix" style="display: none;"></span><span class="content">求解控制问题</span><span class="suffix" style="display: none;"></span></h4>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">与动态规划中 value iteration 思路类似， 每轮迭代先做策略评估，计算出价值<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_k(s)" style="display: block; margin: 0 auto; max-width: 100%;"></span>，然后基于据一定的方法（比如贪婪法）更新当前策略<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi" style="display: block; margin: 0 auto; max-width: 100%;"></span>，最后得到最优价值函敘 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_{*}" style="display: block; margin: 0 auto; max-width: 100%;"></span> 和最优策略 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\pi_{* \circ}" style="display: block; margin: 0 auto; max-width: 100%;"></span></p>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">不同之处：</p>
<ol data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">一般是优化最优动作价值函数<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="q_∗" style="display: block; margin: 0 auto; max-width: 100%;"></span>，而不是状态价值函数<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="v_∗" style="display: block; margin: 0 auto; max-width: 100%;"></span></section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">动态规划一般基于贪婪法更新策略。而蒙特卡罗法一般采用<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\varepsilon" style="display: block; margin: 0 auto; max-width: 100%;"></span>−贪婪法更新。即：
使用 <span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="1-\varepsilon" style="display: block; margin: 0 auto; max-width: 100%;"></span>  的概率贪婪地选择目前认为是最大行为价值的行为，而用<span class="span-inline-equation" style="cursor:pointer"><img class="Formula-image" data-eeimg="true" src alt="\varepsilon" style="display: block; margin: 0 auto; max-width: 100%;"></span> 的概率随机的从所有m 个可选行为中选择。</section></li></ol>
<span class="span-block-equation" style="cursor:pointer" data-tool="mdnice编辑器"><img class="Formula-image" data-eeimg="true" src alt="\pi(a \mid s)=\left\{\begin{array}{ll}
\varepsilon / m+1-\varepsilon &amp; \text { if } a^{*}=\arg \max _{a \in A} Q(s, a) \\
\varepsilon / m &amp; \text { else }
\end{array}\right.
\\" style="display: block; margin: 0 auto; max-width: 100%;"></span>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">MC在线控制-算法伪代码：</p>
<figure data-tool="mdnice编辑器" style="margin: 0; margin-top: 10px; margin-bottom: 10px; display: flex; flex-direction: column; justify-content: center; align-items: center;"><img src="https://raw.githubusercontent.com/weixians/image_bed/master/20210618/montecarlo_presudo.2w8ksn2ev880.png" alt="montecarlo_presudo" width="80%" style="display: block; margin: 0 auto; max-width: 100%;"><figcaption style="margin-top: 5px; text-align: center; color: #888; font-size: 14px;">montecarlo_presudo</figcaption></figure>
<h4 data-tool="mdnice编辑器" style="margin-top: 30px; margin-bottom: 15px; padding: 0px; font-weight: bold; color: black; font-size: 18px;"><span class="prefix" style="display: none;"></span><span class="content">蒙特卡罗优缺点总结：</span><span class="suffix" style="display: none;"></span></h4>
<ol data-tool="mdnice编辑器" style="margin-top: 8px; margin-bottom: 8px; padding-left: 25px; color: black; list-style-type: decimal;">
<li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">优点：不需要环境模型(状态间概率转换模型)</section></li><li><section style="margin-top: 5px; margin-bottom: 5px; line-height: 26px; text-align: left; color: rgb(1,1,1); font-weight: 500;">缺点：需要任务能够结束才能得到return</section></li></ol>
<p data-tool="mdnice编辑器" style="font-size: 16px; padding-top: 8px; padding-bottom: 8px; margin: 0; line-height: 26px; color: black;">下一篇：<a href="https://zhuanlan.zhihu.com/p/382224733" style="text-decoration: none; color: #1e6bb8; word-wrap: break-word; font-weight: bold; border-bottom: 1px solid #1e6bb8;">《强化学习基础》- 时序差分(TD)、SARSA、Q-learning</a></p>
</section>